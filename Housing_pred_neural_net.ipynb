{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing_pred_neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonitorta/Ejercicios_Machine_learning./blob/main/Housing_pred_neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### House price prediction with neural net\n"
      ],
      "metadata": {
        "id": "foTD-evmMPJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "print(f\"python version {sys.version}\")\n",
        "print(f\"numpy version {np.__version__}\")\n",
        "print(f\"pandas version {pd.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf3rV8v4MGk2",
        "outputId": "969cded2-c61f-40dc-9112-b0098f6fef9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python version 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "numpy version 1.21.6\n",
            "pandas version 1.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba4rqvDKrHy6"
      },
      "source": [
        "from tensorflow.keras.datasets import boston_housing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcQJr8ifUkg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5930c16a-1b63-496d-aa4d-9de3cc55f25d"
      },
      "source": [
        "(train_data, train_targets), (test_data, test_targets) = (boston_housing.load_data() )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe2P55KkU0q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31da7f7-70cd-4b71-df41-8c5919afbb11"
      },
      "source": [
        "print(f\"train dataset shape is {train_data.shape}\")\n",
        "print(f\"test dataset shape is {test_data.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset shape is (404, 13)\n",
            "test dataset shape is (102, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnYT734YU2V-"
      },
      "source": [
        "#Let's perform standarization\n",
        "mean = train_data.mean(axis = 0) \n",
        "#substract mean from each row \n",
        "train_data -=mean\n",
        "#compute the std\n",
        "std = train_data.std(axis = 0)\n",
        "#divide with the std\n",
        "train_data /= std\n",
        "#Do same for test data\n",
        "test_data -=mean\n",
        "test_data /=std"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv_gf5NUU6Dw"
      },
      "source": [
        "def build_model():\n",
        "  \"\"\"\n",
        "  Create a neural net of 3 dense layers(all layers are conected), the optimizer is rmsprop that is\n",
        "  a variation of gradient desent with variational learning rate, loss function is mean square error\n",
        "  because is a good one in regression models.\n",
        "  \"\"\"\n",
        "  model = keras.Sequential([\n",
        "                layers.Dense(64, activation = \"relu\"),\n",
        "                layers.Dense(64, activation = \"relu\"),\n",
        "                layers.Dense(1)\n",
        "  ])\n",
        "  model.compile(optimizer = \"rmsprop\", loss = \"mse\", metrics = [\"mae\"] )\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJideSnFU774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db3d53f-9b00-4cbe-e204-77ecb92fdff8"
      },
      "source": [
        "#Here I implement a k-fold validation\n",
        "#Number of folds\n",
        "k = 3\n",
        "#Divide how many samples per fold\n",
        "num_val_samples = len(train_data)//k\n",
        "num_epoch = 1000\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "  print(f\"Processing fold #{i}\")\n",
        "  #Take a slice from train_data and train_targets\n",
        "  val_data = train_data[i*num_val_samples:(i+1)*num_val_samples]\n",
        "  val_targets = train_targets[i*num_val_samples:(i+1)*num_val_samples]\n",
        "  #Tale the others k-1 folds and concatenate them\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i*num_val_samples],\n",
        "       train_data[(i+1)*num_val_samples:]], axis = 0\n",
        "  )\n",
        "  partial_train_samples = np.concatenate(\n",
        "      [train_targets[:i*num_val_samples],\n",
        "       train_targets[(i+1)*num_val_samples:]], axis = 0\n",
        "  )\n",
        "  #Create the model\n",
        "  model = build_model()\n",
        "  #Fit the model with the partial data\n",
        "  model.fit(partial_train_data, partial_train_samples,\n",
        "            epochs = num_epoch, batch_size = 16, verbose = 0 )\n",
        "  #Evaluate the model with the val data\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose = 0)\n",
        "  #Save the mae\n",
        "  all_scores.append(val_mae)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold #0\n",
            "Processing fold #1\n",
            "Processing fold #2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVp8LZ-GZyNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b18cb8-b7b5-4171-da06-2f66b7246f25"
      },
      "source": [
        "#The average error is $2700\n",
        "np.mean(all_scores)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.765585025151571"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZImCVV4VBgU"
      },
      "source": [
        "predictions = model.predict(test_data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4AmLWTSkWD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}